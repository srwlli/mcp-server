# How to Use .coderef/ Structure - Agent Workflow Guide

**Document Version:** 1.0.0
**Created:** 2025-12-31
**Purpose:** Practical guide for AI agents to leverage .coderef/ outputs during implementation
**Status:** âœ… Production Guide

---

## Quick Start

### What is .coderef/?

The `.coderef/` directory contains **static code intelligence files** generated by scanning your project. It provides agents with instant access to architectural patterns, dependencies, and code structure without running expensive scans.

**Key principle:** Use .coderef/ files for quick lookups, use MCP tools for real-time analysis.

---

## The Intelligence Hub

All 5 MCP servers now have centralized scan data at:

```
C:/Users/willh/.mcp-servers/coderef/intelligence/
â”œâ”€â”€ coderef-context/index.json      # 126 elements
â”œâ”€â”€ coderef-docs/index.json          # 54,881 elements
â”œâ”€â”€ coderef-workflow/index.json      # 1,844 elements
â”œâ”€â”€ coderef-personas/index.json      # 1,730 elements
â””â”€â”€ coderef-testing/index.json       # 1,095 elements

Total: 59,676 code elements discovered!
```

**Why centralized?**
- Single source of truth for all agents
- Easy cross-server analysis
- Consistent data structure
- Simple backup/version control

---

## Step-by-Step Workflow

### Step 1: Check If Data Exists

**Before implementing any feature, check if intelligence data is available:**

```python
from pathlib import Path
import json
import sys

# Add coderef utils to path
sys.path.insert(0, "C:/Users/willh/.mcp-servers")
from coderef.utils import check_coderef_available, read_coderef_output

project_path = "C:/Users/willh/.mcp-servers/coderef-workflow"

# Quick check
if check_coderef_available(project_path):
    print("âœ… .coderef/ data available!")
    # Proceed with intelligent implementation
else:
    print("âš ï¸ No .coderef/ data - generate first")
    # Fall back to basic implementation
```

### Step 2: Read What You Need

**Choose the right file for your task:**

| Task | Read This File | Example |
|------|----------------|---------|
| "What exists in this project?" | `index.json` | Find all functions, classes, components |
| "What patterns does this project use?" | `reports/patterns.json` | React hooks, error handling styles |
| "What code changed recently?" | `reports/drift.json` | Files modified since last scan |
| "How complex is this component?" | `reports/complexity-{name}.json` | LOC, cyclomatic complexity |
| "What imports this module?" | Use MCP tool `coderef_query` | Real-time dependency graph |

**Example: Understanding Project Structure**

```python
# Read index from intelligence hub
index_file = Path("C:/Users/willh/.mcp-servers/coderef/intelligence/coderef-workflow/index.json")

with open(index_file, encoding='utf-8') as f:
    index = json.load(f)

# Group by type
by_type = {}
for element in index:
    elem_type = element.get('type', 'unknown')
    by_type[elem_type] = by_type.get(elem_type, 0) + 1

print(f"Project has {len(index)} elements:")
for elem_type, count in by_type.items():
    print(f"  - {elem_type}: {count}")

# Output:
# Project has 1844 elements:
#   - function: 1203
#   - class: 428
#   - component: 213
```

**Example: Finding Reference Components**

```python
# Task: "Implement a new planning generator similar to existing ones"

# Read index
index = json.load(open(index_file, 'utf-8'))

# Find all generator classes
generators = [e for e in index if 'generator' in e['name'].lower() and e['type'] == 'class']

print(f"Found {len(generators)} existing generators to follow as examples:")
for gen in generators:
    print(f"  - {gen['name']} in {gen['file']} (line {gen['line']})")

# Output:
# Found 3 existing generators:
#   - PlanningGenerator in generators/planning_generator.py (line 23)
#   - FoundationGenerator in generators/foundation_generator.py (line 45)
#   - ChangelogGenerator in generators/changelog_generator.py (line 18)
```

### Step 3: Use MCP Tools for Dynamic Analysis

**When you need real-time data, use MCP tools:**

```python
# Static data (fast, from files)
index = read_coderef_output(project_path, 'index')  # Read .coderef/index.json

# Dynamic analysis (slower, but always current)
impact = await call_tool("coderef_context", "coderef_impact", {
    "project_path": project_path,
    "element": "PlanningGenerator",
    "operation": "refactor"
})
# Returns: What breaks if I change PlanningGenerator right now

dependencies = await call_tool("coderef_context", "coderef_query", {
    "project_path": project_path,
    "query_type": "imports-me",
    "target": "PlanningGenerator"
})
# Returns: What files import PlanningGenerator right now
```

### Step 4: When to Refresh Data

**.coderef/ data becomes stale after code changes. Refresh when:**

1. **Major code changes** - New files, renamed modules, refactoring
2. **Before planning workflows** - Ensure latest architecture for `/create-workorder`
3. **After completing features** - Update docs with current state
4. **Drift detected** - When drift.json shows >10% changes

**Check if refresh needed:**

```bash
# From CLI
coderef drift C:/path/to/project --json -i .coderef/index.json

# From Python
drift = read_coderef_output(project_path, 'drift')
changed_pct = (len(drift['changed_files']) / drift['total_files']) * 100

if changed_pct > 10:
    print(f"âš ï¸ {changed_pct:.1f}% drift detected - refresh recommended")
    # Re-run scan
```

**How to refresh:**

```bash
# Quick scan (just index.json, ~5-10 seconds)
cd /c/Users/willh/.mcp-servers/coderef
./scripts/scan-all.py /path/to/project

# Full structure (all 16 outputs, ~30-60 seconds)
python scripts/populate-coderef.py /path/to/project
```

---

## Common Use Cases

### Use Case 1: Planning a New Feature

**Goal:** Understand architecture before implementing dark mode toggle

```python
# 1. Check what theme-related code exists
index = json.loads(read_file("coderef/intelligence/coderef-workflow/index.json"))
theme_elements = [e for e in index if 'theme' in e['name'].lower()]

print(f"Found {len(theme_elements)} theme-related elements:")
for elem in theme_elements[:5]:  # Show first 5
    print(f"  - {elem['type']} {elem['name']} in {elem['file']}")

# 2. Check patterns (how does this project handle themes?)
patterns = json.loads(read_file("coderef/intelligence/coderef-workflow/reports/patterns.json"))
theme_patterns = [p for p in patterns.get('patterns', []) if 'theme' in p['name'].lower()]

if theme_patterns:
    print(f"âœ… Project uses {theme_patterns[0]['name']} - follow this pattern")
else:
    print("â„¹ï¸ No existing theme system - design from scratch")

# 3. Implement extending existing system (not rebuilding!)
```

### Use Case 2: Safe Refactoring

**Goal:** Rename function without breaking dependencies

```python
# 1. Check static data - what calls this function?
index = json.loads(read_file(".coderef/index.json"))
target_function = next(e for e in index if e['name'] == 'generatePlan')

print(f"Function location: {target_function['file']}:{target_function['line']}")

# 2. Use MCP tool for real-time impact analysis
impact = await call_tool("coderef_context", "coderef_impact", {
    "project_path": project_path,
    "element": "generatePlan",
    "operation": "refactor"
})

print(f"âš ï¸ Refactoring will affect {impact['affected_files']} files:")
for file in impact['ripple_effects'][:5]:
    print(f"  - {file}")

# 3. Plan refactoring with full knowledge of dependencies
```

### Use Case 3: Finding Test Gaps

**Goal:** Prioritize testing for high-risk, untested code

```python
# 1. Read complexity data
complexity = json.loads(read_file(".coderef/reports/complexity.json"))

# 2. Read coverage data
coverage = json.loads(read_file(".coderef/reports/coverage.json"))

# 3. Find high-complexity, low-coverage files
high_risk = []
for file, metrics in complexity.items():
    if metrics['cyclomatic_complexity'] > 10:  # Complex
        if coverage.get(file, 0) < 0.5:  # Low coverage
            high_risk.append({
                'file': file,
                'complexity': metrics['cyclomatic_complexity'],
                'coverage': coverage.get(file, 0)
            })

print(f"âš ï¸ {len(high_risk)} high-risk files need testing:")
for item in sorted(high_risk, key=lambda x: x['complexity'], reverse=True)[:5]:
    print(f"  - {item['file']}: complexity={item['complexity']}, coverage={item['coverage']:.0%}")
```

### Use Case 4: Generating Documentation

**Goal:** Auto-populate ARCHITECTURE.md with real data

```python
# 1. Read comprehensive context
context = json.loads(read_file(".coderef/context.json"))

# 2. Extract key sections
architecture_md = f"""
# Architecture

## Overview
{context['overview']}

## Key Components
"""

for component in context['key_components'][:10]:
    architecture_md += f"- **{component['name']}** ({component['type']}): {component['description']}\n"

architecture_md += f"""

## Dependency Graph
```mermaid
{read_file(".coderef/diagrams/dependencies.mmd")}
```

## Patterns in Use
"""

patterns = json.loads(read_file(".coderef/reports/patterns.json"))
for pattern in patterns['patterns']:
    architecture_md += f"- **{pattern['name']}**: Used {pattern['usage']} times\n"

# 3. Write to ARCHITECTURE.md
write_file("ARCHITECTURE.md", architecture_md)
```

---

## File Reference Guide

### Core Files (Always Generated)

| File | Size | Contents | Use For |
|------|------|----------|---------|
| `index.json` | 100KB-4MB | All functions, classes, components with locations | "What exists?", inventory, search |
| `graph.json` | 500KB-6MB | Dependency relationships (imports, calls) | "What depends on what?", refactoring |
| `context.json` | 500KB | Comprehensive project overview | Documentation, onboarding |
| `context.md` | 50-100KB | Human-readable overview | Quick reference, README |

### Reports Directory (Generated on Demand)

| File | Size | Contents | Use For |
|------|------|----------|---------|
| `patterns.json` | 50-100KB | Code patterns (React hooks, error handling) | Following conventions |
| `coverage.json` | 100-200KB | Test coverage by file | Finding gaps, prioritizing tests |
| `validation.json` | 10-50KB | CodeRef2 tag validation | Quality checks |
| `drift.json` | 50-100KB | Changed files since last scan | Knowing when to refresh |
| `complexity-{element}.json` | 1-5KB each | Per-element complexity metrics | Effort estimation |
| `impact-{element}.json` | 5-20KB each | Per-element impact analysis | Risk assessment |

### Diagrams Directory (Visual Exports)

| File | Size | Format | Use For |
|------|------|--------|---------|
| `dependencies.mmd` | 10-50KB | Mermaid | README, docs (GitHub renders) |
| `dependencies.dot` | 10-50KB | GraphViz | Complex visualizations |
| `calls.mmd` | 5-20KB | Mermaid | Call graph visualization |
| `imports.mmd` | 5-20KB | Mermaid | Import dependency graph |

---

## Integration with Workflows

### 1. Planning Workflow (coderef-workflow)

**When:** Running `/create-workorder` or `/analyze-for-planning`

**Reads:**
- `intelligence/coderef-{project}/index.json` â†’ Project inventory
- `intelligence/coderef-{project}/reports/patterns.json` â†’ Existing conventions
- `intelligence/coderef-{project}/context.json` â†’ Architecture overview

**Example:**
```python
# In planning_analyzer.py (INTEGRATE-001)
index_data = read_coderef_output(project_path, 'index')
patterns = read_coderef_output(project_path, 'patterns')

plan['preparation']['inventory'] = {
    'total_elements': len(index_data),
    'functions': len([e for e in index_data if e['type'] == 'function']),
    'classes': len([e for e in index_data if e['type'] == 'class']),
    'patterns': [p['name'] for p in patterns.get('patterns', [])]
}
```

### 2. Documentation Workflow (coderef-docs)

**When:** Running `/generate-docs` or `/coderef-foundation-docs`

**Reads:**
- `index.json` â†’ Component/API inventory
- `context.json` â†’ Architecture description
- `diagrams/*.mmd` â†’ Visual representations

**Example:**
```python
# In foundation_generator.py (INTEGRATE-002)
index = read_coderef_output(project_path, 'index')
diagrams = read_file(".coderef/diagrams/dependencies.mmd")

readme_content = f"""
# {project_name}

## Architecture
```mermaid
{diagrams}
```

## Components ({len(index)} total)
... auto-generated from index.json ...
"""
```

### 3. Persona Workflow (coderef-personas)

**When:** Activating Ava, Marcus, Quinn, or other personas

**Reads:**
- `reports/patterns.json` â†’ Project-specific conventions

**Example:**
```python
# In persona system prompt (INTEGRATE-003)
patterns = read_coderef_output(project_path, 'patterns')

system_prompt += f"""
This project uses the following patterns:
{json.dumps([p['name'] for p in patterns['patterns']], indent=2)}

Follow these conventions when implementing features.
"""
```

### 4. Testing Workflow (coderef-testing)

**When:** Running `/run-tests` with impact analysis

**Reads:**
- `reports/drift.json` â†’ Changed files
- Uses impact to map source â†’ test files

**Example:**
```python
# In test_runner.py (INTEGRATE-004)
drift = read_coderef_output(project_path, 'drift')
changed_files = drift.get('changed_files', [])

# Map to test files
test_files = map_source_to_tests(changed_files)
print(f"Running {len(test_files)} impacted tests (not all {total_tests})")
```

---

## Best Practices

### âœ… DO

1. **Check availability first** - Use `check_coderef_available()` before reading
2. **Use UTF-8 encoding** - Always `open(file, encoding='utf-8')` to handle emojis
3. **Fall back gracefully** - If .coderef/ missing, use MCP tools or basic heuristics
4. **Read static, call dynamic** - Files for structure, MCP tools for real-time impact
5. **Refresh after major changes** - Keep data current for accurate analysis

### âŒ DON'T

1. **Don't modify .coderef/ directly** - Always regenerate with coderef CLI
2. **Don't assume it exists** - Always check first, have fallback
3. **Don't use stale data for critical decisions** - Check drift before refactoring
4. **Don't re-scan unnecessarily** - Static files are fast, use them!
5. **Don't hardcode paths** - Use wrapper utilities that handle hub structure

---

## Wrapper Utilities Reference

All 5 MCP servers have access to `coderef/utils/` helpers:

```python
# Import (from any MCP server)
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from coderef.utils import check_coderef_available, read_coderef_output

# Check if data exists
if check_coderef_available(project_path):
    # Data available
    pass

# Read common outputs
index = read_coderef_output(project_path, 'index')        # .coderef/index.json
graph = read_coderef_output(project_path, 'graph')        # .coderef/graph.json
patterns = read_coderef_output(project_path, 'patterns')  # .coderef/reports/patterns.json
coverage = read_coderef_output(project_path, 'coverage')  # .coderef/reports/coverage.json
drift = read_coderef_output(project_path, 'drift')        # .coderef/reports/drift.json
```

**Three-tier fallback pattern:**
1. Try reading from `.coderef/` (fast, cached)
2. Try calling MCP tool (dynamic, slower)
3. Fall back to basic heuristics (last resort)

---

## Troubleshooting

### Problem: "File not found" when reading .coderef/ files

**Solution:**
```python
# Check if data exists first
if not check_coderef_available(project_path):
    print("âš ï¸ No .coderef/ data - generating...")
    # Run scan
    subprocess.run(["coderef", "scan", project_path, "--output", ".coderef/index.json"])
```

### Problem: "JSON decode error" when reading files

**Cause:** Coderef CLI outputs emoji/text prefix before JSON

**Solution:**
```python
def read_index_json(file_path):
    with open(file_path, encoding='utf-8') as f:
        content = f.read()
        # Strip emoji/text prefix
        if 'ðŸ”' in content or 'Scanning' in content:
            bracket_pos = content.find('[')
            brace_pos = content.find('{')
            positions = [p for p in [bracket_pos, brace_pos] if p >= 0]
            if positions:
                json_start = min(positions)
                content = content[json_start:]
        return json.loads(content)
```

### Problem: Data is stale / doesn't reflect recent changes

**Solution:**
```bash
# Check drift
coderef drift /path/to/project --json -i .coderef/index.json

# If drift > 10%, refresh
python scripts/populate-coderef.py /path/to/project
```

---

## Performance Tips

### Loading Large Files

For projects with 100K+ elements (like coderef-docs with 116K):

```python
# Slow: Load entire index, then filter
index = json.loads(read_file("index.json"))  # 4MB, 200ms
functions = [e for e in index if e['type'] == 'function']

# Fast: Stream and filter
import ijson
functions = []
with open("index.json", 'rb') as f:
    parser = ijson.items(f, 'item')
    for element in parser:
        if element['type'] == 'function':
            functions.append(element)
            if len(functions) >= 100:  # Stop after 100
                break
```

### Caching in Memory

```python
# Cache index in memory for repeated access
_index_cache = {}

def get_index(project_path):
    if project_path not in _index_cache:
        _index_cache[project_path] = read_coderef_output(project_path, 'index')
    return _index_cache[project_path]

# Use cached version
index = get_index(project_path)  # First call: 200ms
index = get_index(project_path)  # Subsequent: <1ms
```

---

## Summary

**Key Takeaways:**

1. **.coderef/ is your static intelligence** - Fast lookups, no subprocess overhead
2. **MCP tools are dynamic analysis** - Real-time dependencies, impact, complexity
3. **Intelligence hub centralizes data** - 59,676 elements across 5 servers
4. **Always check availability** - Fall back gracefully if data missing
5. **Refresh when stale** - Use drift detection to know when
6. **Use wrapper utilities** - Consistent three-tier fallback pattern

**Quick Reference:**
- Check: `check_coderef_available(project_path)`
- Read: `read_coderef_output(project_path, 'index')`
- Refresh: `python scripts/populate-coderef.py /path/to/project`
- Hub location: `C:/Users/willh/.mcp-servers/coderef/intelligence/`

---

**Document Status:** âœ… Complete Usage Guide
**Version:** 1.0.0
**For:** AI Agents implementing features with code intelligence
**Maintained by:** WO-CODEREF-OUTPUT-UTILIZATION-001
