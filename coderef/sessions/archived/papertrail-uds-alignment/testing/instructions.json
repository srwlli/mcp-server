{
  "session_id": "papertrail-uds-alignment-testing",
  "phase": "Testing Phase: Deferred Unit Tests",
  "created": "2026-01-10",
  "purpose": "Implement 3 deferred testing tasks from WO-PAPERTRAIL-SCHEMA-ADDITIONS-001 to achieve >= 90% test coverage for new validators",

  "orchestrator_instructions": {
    "task": "Wait for coderef-testing agent to complete all 3 test tasks, verify test coverage >= 90%, update parent session status",
    "steps": [
      "Read testing-completion-report.json from coderef-testing agent",
      "Verify all 3 test tasks completed (TEST-001, TEST-002, TEST-003)",
      "Verify test coverage >= 90% for AnalysisValidator and ExecutionLogValidator",
      "Create orchestrator-testing-report.md with summary",
      "Update parent session communication.json status",
      "Mark WO-PAPERTRAIL-SCHEMA-ADDITIONS-001 as fully complete"
    ],
    "success_criteria": [
      "All 15 test cases passing",
      "Test coverage >= 90% for new validators",
      "ValidatorFactory integration tests passing"
    ]
  },

  "agent_instructions": {
    "agent_id": "coderef-testing",
    "task": "Implement 3 deferred testing tasks for Papertrail validators",
    "context": {
      "parent_workorder": "WO-PAPERTRAIL-SCHEMA-ADDITIONS-001",
      "implementation_complete": "Core implementation finished, only tests deferred",
      "files_to_test": [
        "papertrail/validators/analysis.py (AnalysisValidator)",
        "papertrail/validators/execution_log.py (ExecutionLogValidator)",
        "papertrail/validators/factory.py (ValidatorFactory)"
      ],
      "sample_files_location": "C:\\Users\\willh\\.mcp-servers\\papertrail\\samples\\"
    },
    "instructions": [
      "TEST-001: Create AnalysisValidator Unit Tests",
      "========================================",
      "File: papertrail/tests/test_validators.py",
      "Class: TestAnalysisValidator (add new class)",
      "Estimated effort: 2-3 hours",
      "",
      "Test cases to implement (6 tests):",
      "1. test_valid_analysis_json",
      "   - Load valid analysis.json sample",
      "   - Validate with AnalysisValidator",
      "   - Assert score >= 90",
      "   - Assert no CRITICAL or MAJOR errors",
      "",
      "2. test_invalid_analysis_missing_required",
      "   - Create analysis.json missing 'project_path' field",
      "   - Validate",
      "   - Assert score < 90",
      "   - Assert CRITICAL error for missing required field",
      "",
      "3. test_invalid_analysis_wrong_type",
      "   - Create analysis.json with invalid 'project_type' enum value",
      "   - Validate",
      "   - Assert score < 90",
      "   - Assert MAJOR error for invalid enum",
      "",
      "4. test_uds_metadata_validation",
      "   - Create analysis.json with invalid UDS section",
      "   - Validate",
      "   - Assert UDS metadata validated correctly",
      "",
      "5. test_inventory_consistency",
      "   - Create analysis.json where total_elements != sum of by_type counts",
      "   - Validate",
      "   - Assert MAJOR error for inventory mismatch",
      "",
      "6. test_tech_stack_warnings",
      "   - Create analysis.json with >= 3 'unknown' values in tech_stack",
      "   - Validate",
      "   - Assert WARNING issued",
      "",
      "---",
      "",
      "TEST-002: Create ExecutionLogValidator Unit Tests",
      "==============================================",
      "File: papertrail/tests/test_validators.py",
      "Class: TestExecutionLogValidator (add new class)",
      "Estimated effort: 3-4 hours",
      "",
      "Test cases to implement (9 tests):",
      "1. test_valid_execution_log",
      "   - Load valid execution-log.json sample",
      "   - Validate with ExecutionLogValidator",
      "   - Assert score >= 90",
      "",
      "2. test_invalid_missing_workorder_id",
      "   - Create execution-log.json missing 'workorder_id'",
      "   - Assert CRITICAL error",
      "",
      "3. test_invalid_task_status_enum",
      "   - Create execution-log.json with invalid status value",
      "   - Assert MAJOR error for invalid enum",
      "",
      "4. test_workorder_id_format",
      "   - Test valid format: WO-AUTH-SYSTEM-001",
      "   - Test invalid formats: WO-001, AUTH-001, etc.",
      "   - Assert format validation works",
      "",
      "5. test_feature_name_format",
      "   - Test valid kebab-case: 'my-feature-name'",
      "   - Test invalid formats: 'MyFeature', 'my_feature', etc.",
      "   - Assert format validation works",
      "",
      "6. test_task_count_mismatch",
      "   - Create execution-log.json where task_count != len(tasks)",
      "   - Assert MAJOR error for mismatch",
      "",
      "7. test_cross_validation_valid",
      "   - Create execution-log.json with task_ids referencing valid plan.json",
      "   - Validate with cross-validation enabled",
      "   - Assert no cross-validation errors",
      "",
      "8. test_cross_validation_invalid",
      "   - Create execution-log.json with task_id not in plan.json",
      "   - Validate with cross-validation enabled",
      "   - Assert MAJOR error for orphaned task_id",
      "",
      "9. test_cross_validation_missing_plan",
      "   - Create execution-log.json without corresponding plan.json",
      "   - Validate with cross-validation enabled",
      "   - Assert graceful fallback (WARNING, not CRITICAL)",
      "",
      "---",
      "",
      "TEST-003: Create ValidatorFactory Integration Tests",
      "================================================",
      "File: papertrail/tests/test_factory.py",
      "Methods: Add to existing TestValidatorFactory class",
      "Estimated effort: 2-3 hours",
      "",
      "Test cases to implement (5 tests):",
      "1. test_analysis_json_detection",
      "   - Path: /coderef/workorder/my-feature/analysis.json",
      "   - Call ValidatorFactory.get_validator(path)",
      "   - Assert returns AnalysisValidator instance",
      "",
      "2. test_execution_log_detection",
      "   - Path: /coderef/workorder/my-feature/execution-log.json",
      "   - Call ValidatorFactory.get_validator(path)",
      "   - Assert returns ExecutionLogValidator instance",
      "",
      "3. test_path_pattern_matching",
      "   - Test various path formats",
      "   - Assert patterns match correctly",
      "   - Test edge cases (nested directories, different separators)",
      "",
      "4. test_end_to_end_analysis_validation",
      "   - Create valid analysis.json file on disk",
      "   - Use ValidatorFactory to auto-detect and validate",
      "   - Assert score >= 90 without manual validator instantiation",
      "",
      "5. test_end_to_end_execution_log_validation",
      "   - Create valid execution-log.json + plan.json on disk",
      "   - Use ValidatorFactory to auto-detect and validate with cross-validation",
      "   - Assert score >= 90 and cross-validation passes",
      "",
      "---",
      "",
      "IMPLEMENTATION STEPS:",
      "====================",
      "1. Read existing test files:",
      "   - papertrail/tests/test_validators.py (for structure/patterns)",
      "   - papertrail/tests/test_factory.py (for ValidatorFactory tests)",
      "",
      "2. Review validators to understand validation logic:",
      "   - papertrail/validators/analysis.py",
      "   - papertrail/validators/execution_log.py",
      "   - papertrail/validators/factory.py",
      "",
      "3. Review sample files for test data:",
      "   - C:\\Users\\willh\\.mcp-servers\\papertrail\\samples\\analysis.json",
      "   - C:\\Users\\willh\\.mcp-servers\\papertrail\\samples\\execution-log.json",
      "",
      "4. Implement TEST-001 (AnalysisValidator tests)",
      "   - Add TestAnalysisValidator class",
      "   - Implement all 6 test methods",
      "   - Run tests: pytest papertrail/tests/test_validators.py::TestAnalysisValidator",
      "",
      "5. Implement TEST-002 (ExecutionLogValidator tests)",
      "   - Add TestExecutionLogValidator class",
      "   - Implement all 9 test methods",
      "   - Run tests: pytest papertrail/tests/test_validators.py::TestExecutionLogValidator",
      "",
      "6. Implement TEST-003 (ValidatorFactory integration tests)",
      "   - Add 5 test methods to existing TestValidatorFactory",
      "   - Run tests: pytest papertrail/tests/test_factory.py",
      "",
      "7. Run full test suite:",
      "   - pytest papertrail/tests/ --cov=papertrail/validators --cov-report=term-missing",
      "   - Verify coverage >= 90% for analysis.py and execution_log.py",
      "",
      "8. Create testing-completion-report.json with:",
      "   {",
      "     \"tests_implemented\": 20,",
      "     \"tests_passing\": 20,",
      "     \"coverage\": {",
      "       \"analysis_validator\": \"XX%\",",
      "       \"execution_log_validator\": \"XX%\",",
      "       \"validator_factory\": \"XX%\"",
      "     },",
      "     \"effort_hours\": 7.5,",
      "     \"status\": \"complete\"",
      "   }",
      "",
      "9. Update communication.json status to 'complete'",
      "",
      "SUCCESS CRITERIA:",
      "=================",
      "- All 20 test cases implemented and passing",
      "- Test coverage >= 90% for AnalysisValidator",
      "- Test coverage >= 90% for ExecutionLogValidator",
      "- ValidatorFactory integration tests passing",
      "- Cross-validation tests verify task_id â†’ plan.json references",
      "- No test failures or errors"
    ],
    "output_files": [
      "papertrail/tests/test_validators.py (updated with 2 new test classes)",
      "papertrail/tests/test_factory.py (updated with 5 new test methods)",
      "testing-completion-report.json (test results summary)",
      "testing-completion-report.md (human-readable summary)"
    ]
  }
}
