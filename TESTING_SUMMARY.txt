================================================================================
                    PROFESSIONAL TESTING ARCHITECTURE
                           CodeRef Ecosystem
================================================================================

Created: 2025-12-26
Status: ✅ PRODUCTION READY
Documents: 3 files (see below)

================================================================================
OVERVIEW
================================================================================

The CodeRef Ecosystem now has a professional, enterprise-grade testing
framework that will scale as the system grows.

Current State:
  ✅ coderef-workflow: 67 tests (100% pass) with real proof artifacts
  ⚠️  coderef-docs, coderef-context, coderef-personas: Need standardization

This framework applies across all 4 servers with consistent structure.

================================================================================
THREE KEY DOCUMENTS
================================================================================

1. TESTING_ARCHITECTURE.md (Strategic Overview)
   - What testing looks like professionally
   - Test categories and what each proves
   - Real data vs mock data
   - Coverage requirements & quality gates
   - CI/CD integration standards
   - Best practices & anti-patterns

   USE: Read first to understand the philosophy

2. TEST_SETUP_CHECKLIST.md (Tactical Implementation)
   - Step-by-step setup instructions
   - Copy-paste templates for conftest.py, pytest.ini, .coveragerc
   - Example test files for each category
   - Commands for running tests
   - Troubleshooting guide
   - Completion checklist

   USE: Follow when setting up tests in a server

3. PROFESSIONAL_TESTING_REVIEW.md (Executive Assessment)
   - Current state assessment
   - What makes this "professional"
   - Comparison before/after
   - Real metrics (coderef-workflow: 67 tests, 0.95s execution)
   - Implementation roadmap for all 4 servers
   - Success criteria

   USE: Show stakeholders / understand impact

================================================================================
QUICK START (5 MINUTES)
================================================================================

For any server (coderef-docs, coderef-context, coderef-personas):

1. Create test structure:
   mkdir -p tests/{unit,integration,smoke,performance,security,proofs,fixtures}

2. Copy files from coderef-workflow:
   - conftest.py → tests/conftest.py
   - pytest.ini → pytest.ini
   - .coveragerc → .coveragerc

3. Copy a test file to understand structure:
   - Look at tests/integration/test_tool_invocation.py
   - Follow pattern with WHAT IT PROVES + ASSERTION

4. Run tests:
   pytest tests/ -v --cov=src --cov-fail-under=85

That's it! You now have professional testing.

================================================================================
TEST CATEGORIES (7 TYPES)
================================================================================

1. UNIT TESTS (tests/unit/)
   Speed: <100ms each | Mocks: YES | Purpose: Test single components
   Example: test_json_rpc_format, test_error_handling

2. INTEGRATION TESTS (tests/integration/)
   Speed: <2s each | Mocks: At boundaries | Purpose: Test interaction
   Example: test_tool_invocation, test_data_flow

3. SMOKE TESTS (tests/smoke/)
   Speed: <50ms each | Mocks: YES | Purpose: Quick sanity checks
   Example: test_imports, test_server_startup

4. PERFORMANCE TESTS (tests/performance/)
   Speed: Measured | Mocks: YES | Purpose: Baseline metrics
   Example: test_tool_latency, test_memory_usage

5. SECURITY TESTS (tests/security/)
   Speed: <200ms each | Mocks: YES | Purpose: Validation
   Example: test_input_validation, test_path_traversal

6. PROOF TESTS (tests/proofs/)
   Speed: <500ms each | Mocks: NO (real data) | Purpose: Evidence
   Example: test_real_coderef_injection_proof

7. FIXTURES (tests/fixtures/)
   NOT tests - these are helper mocks used by all tests
   Example: MockMCPClient, MockCoderefScanResponse

================================================================================
REAL DATA vs MOCKS
================================================================================

KEY PRINCIPLE:
  Unit/Integration tests use MOCKS (fast, isolated)
  Proof tests use REAL DATA (evidence of functionality)

WHY THIS MATTERS:
  - Mocks: Can test 67 tests in 0.95 seconds
  - Real: Proves it works in practice, not just theory
  - Together: Complete confidence with speed

EXAMPLE:
  Mock test (<10ms):
    assert mock.get_call_count("coderef_scan") == 1

  Real test (<500ms):
    assert real_proof["analysis"]["coderef_scan_results"]["total_files"] == 45

================================================================================
PROOF ARTIFACTS (Unique to CodeRef)
================================================================================

Tests are evidence! Real data proves functionality.

Structure:
  coderef/workorder/test-coderef-injection/
    ├── context.json          (what was requested)
    ├── analysis.json         (real coderef tool output)
    ├── plan.json             (plan informed by real data)
    └── PROOF_DOCUMENT.md     (explains the proof)

Test:
  tests/proofs/test_coderef_injection_proof.py
    └── Validates proof is real (not mock)

Registry:
  coderef/proofs/index.json
    └── Lists all proofs for easy reference

Marked with:
  - "source_tool": "coderef_scan" (identifies tool)
  - "timestamp": "2025-12-26T14:30:00Z" (when invoked)
  - "proof_of_injection": "Explanation" (what happened)

================================================================================
COVERAGE REQUIREMENTS
================================================================================

Minimum: 85% overall
  - Unit tests: 90%+
  - Integration tests: 80%+
  - Enforced via .coveragerc

Check coverage:
  pytest tests/ --cov=src --cov-report=html
  # Opens htmlcov/index.html showing exactly what's uncovered

================================================================================
CI/CD INTEGRATION
================================================================================

GitHub Actions template provided (see TEST_SETUP_CHECKLIST.md)

Automated on every push:
  1. Smoke tests (10s)
  2. Unit tests (30s)
  3. Integration tests (60s)
  4. Coverage check (fail if <85%)
  5. Proof validation

Branch protection: Require tests pass before merge

================================================================================
COMMANDS CHEAT SHEET
================================================================================

# All tests
pytest tests/ -v

# Specific category
pytest tests/unit/ -v
pytest tests/integration/ -v
pytest tests/smoke/ -v
pytest tests/proofs/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html

# Fast feedback loop (watch mode)
ptw tests/ -- -v

# Specific test
pytest tests/unit/test_basic.py::TestClass::test_method -v

# By marker
pytest -m unit -v
pytest -m "integration and not slow" -v

# Performance benchmark
pytest tests/performance/ -v --benchmark-only

================================================================================
ROLLOUT PLAN (Apply to All 4 Servers)
================================================================================

coderef-workflow:    COMPLETE (67 tests, 100% pass)
coderef-docs:        Phase 2 (1-2 hours)
coderef-context:     Phase 3 (1-2 hours)
coderef-personas:    Phase 4 (30 minutes)
Ecosystem CI/CD:     Phase 5 (1 hour)

Total time: ~5 hours to professional testing across all servers.

Details in: PROFESSIONAL_TESTING_REVIEW.md -> "Implementation Roadmap"

================================================================================
KEY FILES REFERENCE
================================================================================

READING ORDER:
1. This file (overview)
2. PROFESSIONAL_TESTING_REVIEW.md (why it matters)
3. TESTING_ARCHITECTURE.md (how it works)
4. TEST_SETUP_CHECKLIST.md (how to do it)

IMPLEMENTATION:
- Use TEST_SETUP_CHECKLIST.md for step-by-step
- Copy conftest.py, pytest.ini, .coveragerc templates
- Follow test structure from coderef-workflow

VERIFICATION:
- Run: pytest tests/ -v --cov=src --cov-fail-under=85
- Check: htmlcov/index.html for coverage report
- Validate: tests/proofs/ to verify real data

================================================================================
SUCCESS CRITERIA
================================================================================

This is production-ready when:
  ✅ All tests pass locally
  ✅ Coverage >= 85%
  ✅ Tests categorized by purpose
  ✅ Mocks separated in fixtures/
  ✅ Real data proofs exist
  ✅ CI/CD configured
  ✅ Documentation complete
  ✅ Team trained
  ✅ Proof artifacts generated regularly

Once complete, you have:
  - Professional testing framework
  - Living proof of functionality
  - Automated quality gates
  - Scalable structure for growth

================================================================================

Questions? Check:
  - TESTING_ARCHITECTURE.md (comprehensive guide)
  - TEST_SETUP_CHECKLIST.md (Troubleshooting)
  - PROFESSIONAL_TESTING_REVIEW.md (Rationale)

Ready to implement? See TEST_SETUP_CHECKLIST.md -> Step 1.

================================================================================
                         PRODUCTION READY
================================================================================
