---
generated_by: coderef-docs
template: readme
date: "2026-01-14T01:20:48Z"
doc_type: readme
feature_id: foundation-docs
workorder_id: foundation-docs-001
task: Generate foundation documentation
agent: Claude Code AI
_uds:
  validation_score: 95
  validated_at: "2026-01-14T01:20:48Z"
  validator: UDSValidator
---

# CodeRef Context MCP Server

**[Version]** 2.0.0 | **[Date]** 2026-01-14 | **[Maintainer]** willh

## Purpose

CodeRef Context is an MCP (Model Context Protocol) server that provides fast, read-only access to pre-scanned code intelligence. It reads from `.coderef/` directory files generated by the CodeRef dashboard or CLI, eliminating subprocess overhead and providing instant code analysis for AI agents.

## Overview

This MCP server exposes 12 code intelligence tools that enable AI agents to:
- Scan and discover code elements (functions, classes, components, hooks)
- Query code relationships (dependencies, imports, call chains)
- Analyze change impact and complexity
- Discover patterns and test coverage gaps
- Generate comprehensive codebase context
- Export data in multiple formats (JSON, JSON-LD, Mermaid, DOT)

**Key Architecture Decision**: Reads from pre-scanned `.coderef/` files instead of calling CLI subprocesses, resulting in 100x faster response times.

## What

### Core Components

- **MCP Server** (`server.py`): Main entry point exposing 12 MCP tools
- **CodeRef Reader** (`src/coderef_reader.py`): Reads and queries `.coderef/` data files
- **Handlers** (`src/handlers_refactored.py`): Async handlers for each MCP tool
- **Export Processor** (`processors/export_processor.py`): Handles export operations

### MCP Tools Exposed

1. `coderef_scan` - Discover all code elements
2. `coderef_query` - Query relationships (calls, imports, dependencies)
3. `coderef_impact` - Analyze change impact
4. `coderef_complexity` - Get complexity metrics
5. `coderef_patterns` - Discover patterns and test gaps
6. `coderef_coverage` - Test coverage analysis
7. `coderef_context` - Generate comprehensive context
8. `coderef_validate` - Validate CodeRef2 references
9. `coderef_drift` - Detect drift between index and code
10. `coderef_incremental_scan` - Incremental scan (only changed files)
11. `coderef_diagram` - Generate dependency diagrams
12. `coderef_tag` - Add CodeRef2 tags to source files
13. `coderef_export` - Export data in various formats
14. `validate_coderef_outputs` - Validate `.coderef/` files against schemas

## Why

**Problem Solved**: AI agents need fast, reliable access to code intelligence without the overhead of CLI subprocess calls.

**Benefits**:
- **100x Faster**: Direct file reads vs subprocess execution
- **Read-Only Safety**: No code modification, only analysis
- **No External Dependencies**: Works with pre-scanned data
- **MCP Standard**: Follows Model Context Protocol for agent integration

## When

Use this server when:
- Building AI agents that need code intelligence
- Integrating CodeRef analysis into MCP-compatible tools
- Requiring fast, read-only code analysis
- Working with pre-scanned codebases (via dashboard or CLI)

## Prerequisites

- Python 3.10+
- `.coderef/` directory with pre-scanned data (generated by coderef-dashboard scanner):
  - `index.json` (required) - v2.0.0 format with metadata
  - `graph.json` (required) - v2.0.0 format
  - `context.json` (optional) - Project overview
  - Optional: `patterns.json`, `coverage.json`, `diagrams/`, `exports/`

**Scanner:** Install coderef-dashboard from `C:\Users\willh\Desktop\coderef-dashboard\`

## Installation

```bash
# Navigate to project directory
cd C:\Users\willh\.mcp-servers\coderef-context

# Install dependencies
pip install -e .

# Generate .coderef/ data using production scanner
# (From coderef-dashboard/packages/coderef-core)
cd C:\Users\willh\Desktop\coderef-dashboard\packages\coderef-core
npm run scan -- --project-path /path/to/your/project
```

## Quick Start

### 1. Verify CodeRef Data

```bash
# Check if .coderef/ directory exists
ls .coderef/

# Should contain:
# - index.json
# - graph.json
# - context.json
```

### 2. Start MCP Server

```bash
# Run via stdio (for MCP clients)
python server.py

# Or configure in MCP client settings
```

### 3. Use Tools

Example: Scan codebase
```json
{
  "name": "coderef_scan",
  "arguments": {
    "project_path": "/absolute/path/to/project"
  }
}
```

## Usage Examples

### Example 1: Discover All Code Elements

```python
# MCP tool call
{
  "name": "coderef_scan",
  "arguments": {
    "project_path": "/path/to/project",
    "languages": ["ts", "tsx", "js", "jsx"],
    "use_ast": true
  }
}

# Returns: List of all functions, classes, components, hooks
```

### Example 2: Query Code Relationships

```python
# Find what calls a specific function
{
  "name": "coderef_query",
  "arguments": {
    "project_path": "/path/to/project",
    "query_type": "calls",
    "target": "authenticateUser",
    "max_depth": 3
  }
}
```

### Example 3: Analyze Change Impact

```python
# Check impact of modifying a service
{
  "name": "coderef_impact",
  "arguments": {
    "project_path": "/path/to/project",
    "element": "AuthService",
    "operation": "modify",
    "max_depth": 3
  }
}
```

## Common Issues & Troubleshooting

### Issue: "No scan data found"

**Error Message**:
```json
{
  "success": false,
  "error": "No scan data found. Run scan first to create .coderef/ directory."
}
```

**Resolution**:
1. Run CodeRef scanner via dashboard or CLI
2. Ensure `.coderef/` directory exists in project root
3. Verify `index.json`, `graph.json`, and `context.json` are present

### Issue: "CodeRef data not found: graph.json"

**Error Message**: `FileNotFoundError: CodeRef data not found: graph.json`

**Resolution**:
- Re-run CodeRef scan to generate missing files
- Check file permissions on `.coderef/` directory

### Issue: Corrupted JSON files

**Error**: JSON parsing errors when reading `.coderef/` files

**Resolution**:
- Delete corrupted files and re-scan
- Check disk space and file system integrity

## Schema Version Support

**Version 2.1.1+** supports both legacy v1.0.0 and production v2.0.0 .coderef/ schema formats with automatic detection and normalization.

### Schema Formats

#### v2.0.0 Schema (Production Standard)
**Generated by:** coderef-dashboard/packages/coderef-core (current production scanner)

```json
// index.json
{
  "version": "2.0.0",
  "generatedAt": "2026-01-22T10:00:00.000Z",
  "projectPath": "/path/to/project",
  "totalElements": 780,
  "elementsByType": {
    "component": 350,
    "method": 306,
    "function": 77,
    "constant": 27
  },
  "elements": [
    {"name": "func1", "type": "function", "file": "test.py", "line": 10},
    {"name": "func2", "type": "function", "file": "test.py", "line": 20}
  ]
}

// graph.json
{
  "version": "2.0.0",
  "nodes": [
    {"id": "node1", "name": "func1"},
    {"id": "node2", "name": "func2"}
  ],
  "edges": {"node1": ["node2"]}
}
```

**Benefits:**
- Self-documenting (includes timestamp, project path, element counts)
- AI-friendly metadata for context-aware analysis
- Future-proof structure for additional metadata

#### v1.0.0 Schema (Legacy - Backward Compatibility Only)
**Status:** Deprecated. Supported for backward compatibility with old files.

```json
// index.json (flat array)
[
  {"name": "func1", "type": "function", "file": "test.py", "line": 10},
  {"name": "func2", "type": "function", "file": "test.py", "line": 20}
]

// graph.json
{
  "nodes": {
    "node1": {"id": "node1", "name": "func1"},
    "node2": {"id": "node2", "name": "func2"}
  },
  "edges": {"node1": ["node2"]}
}
```

### How It Works

1. **Automatic Detection**: Server detects schema version on load
   - v2.0.0 (Production): Has `"version": "2.0.0"` field with metadata wrapper
   - v1.0.0 (Legacy): Flat arrays without version field

2. **Transparent Normalization**: v2.0.0 data normalized to v1.0.0 internally for processing
   - Extracts `elements` array from v2.0.0 wrapper
   - Converts v2.0.0 nodes list to v1.0.0 dict format
   - Zero breaking changes - all tools work with both formats

3. **Production Scanner**: coderef-dashboard generates v2.0.0 by default
   - Location: `coderef-dashboard/packages/coderef-core/src/fileGeneration/saveIndex.ts`
   - Includes rich metadata (timestamp, project path, element counts)

### Migration Notes

- **No action required**: Server handles both formats automatically
- **Recommended**: Update to v2.0.0 schema for better metadata support
- **Test coverage**: 23 tests verify both schema versions

## Architecture

See [ARCHITECTURE.md](coderef/foundation-docs/ARCHITECTURE.md) for detailed system design.

## API Reference

See [API.md](coderef/foundation-docs/API.md) for complete MCP tool documentation.

## Data Models

See [SCHEMA.md](coderef/foundation-docs/SCHEMA.md) for data structure documentation.

## Components

See [COMPONENTS.md](coderef/foundation-docs/COMPONENTS.md) for module documentation.

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run integration tests
pytest tests/test_integration.py

# Run with coverage
pytest --cov=src --cov=processors
```

### Code Quality

```bash
# Format code
black src/ processors/ tests/

# Lint code
ruff check src/ processors/ tests/
```

## License

MIT

## References

- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)
- [CodeRef Documentation](https://coderef.dev)
- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)

---

**AI Agent Note**: This MCP server provides read-only code intelligence. All tools are safe to use in automated workflows. For code modification, use `coderef_tag` tool which requires explicit CLI integration.
