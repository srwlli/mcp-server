# coeref-testing - AI Context Documentation

**Project:** coeref-testing (MCP Testing Infrastructure Server)
**Version:** 1.0.0
**Status:** ğŸš§ Development (Planning Phase)
**Created:** 2025-12-26
**Last Updated:** 2025-12-26

---

## Quick Summary

**coeref-testing** is a universal MCP server for test orchestration, execution, and reporting that works with any project and any test framework (pytest, jest, cargo, mocha, vitest, etc.).

**Core Innovation:** Framework-agnostic architecture that auto-detects test frameworks and executes tests with unified result aggregation, enabling cross-project testing from a single MCP server.

**Latest Update (v1.0.0):**
- ğŸš§ Planning phase complete with comprehensive TESTING_GUIDE.md
- ğŸ“‹ 14 core tools designed (discovery, execution, analysis, reporting)
- ğŸ‘¤ testing-expert persona specified (15 expertise areas)
- ğŸ“Š Universal result schema for all frameworks

**Key Relationships:**
- **Independent:** No dependencies on coderef-context, coderef-workflow, coderef-docs, coderef-personas
- **Integrable:** Optional hooks to other MCP servers when available
- **Reusable:** Works with CodeRef ecosystem (4 servers), next-scraper, any user project

Together they form a complete testing ecosystem: Discovery â†’ Execution â†’ Aggregation â†’ Analysis â†’ Reporting.

---

## Problem & Vision

### The Problem
Test infrastructure is scattered. Projects use different frameworks (pytest, jest, cargo). Tests are hard to discover, orchestrate, and report on. No unified view across multiple test frameworks or projects.

### The Solution
Create a universal MCP testing server that: (1) Auto-detects test frameworks, (2) Executes tests uniformly, (3) Aggregates results across frameworks, (4) Analyzes coverage/performance/flakiness, (5) Generates reports in multiple formats.

### How It Works
1. **Discover** - Scan project, detect frameworks (pytest/jest/cargo/mocha/vitest)
2. **Execute** - Run tests using detected framework, async/parallel execution
3. **Aggregate** - Collect results into unified JSON schema
4. **Analyze** - Calculate coverage, detect flaky tests, analyze performance
5. **Report** - Generate markdown/HTML/JSON reports with trends

---

## Architecture

### Core Concepts

**Framework Detection**
Auto-detect test frameworks by scanning project structure:
- Detect pytest (pyproject.toml, tests/ directory, conftest.py)
- Detect jest/vitest (package.json, jest.config.js, vitest.config.ts)
- Detect cargo (Cargo.toml with [dev-dependencies])
- Detect custom runners (shell scripts, make targets)

**Unified Result Schema**
All frameworks output to standard JSON:
```json
{
  "project": "/path", "framework": "pytest",
  "summary": {"total": 247, "passed": 245, "failed": 2},
  "tests": [{"name": "test_X", "status": "passed", "duration": 0.5}]
}
```

**Async/Parallel Execution**
Run tests concurrently with configurable workers, proper isolation, timeout handling.

### Data Flow

```
User Project (any framework)
    â†“
coeref-testing discovers tests
    â†“
Detects framework (pytest/jest/cargo)
    â†“
Executes tests in parallel
    â†“
Collects results â†’ unified JSON
    â†“
Analyzes (coverage/performance/flaky)
    â†“
Generates reports (markdown/HTML/JSON)
    â†“
Archives results with timestamp
```

### Key Integration Points

- **Depends on:** Python async runtime, subprocess execution, git (for result storage)
- **Used by:** Agents (via testing-expert persona), CI/CD pipelines, development workflows
- **Orchestrated via:** MCP tools + slash commands, or direct API calls

---

## Tools Catalog

| Tool | Purpose | Type |
|------|---------|------|
| `discover_tests` | Find tests in project, auto-detect framework | Discovery |
| `list_test_frameworks` | Show detected frameworks + versions | Discovery |
| `run_all_tests` | Execute full test suite | Execution |
| `run_test_file` | Run single test file | Execution |
| `run_test_category` | Run tests matching pattern/tag | Execution |
| `run_tests_in_parallel` | Execute tests with concurrency control | Execution |
| `get_test_results` | Query results (date, status, duration) | Management |
| `aggregate_results` | Summary across all tests | Management |
| `generate_test_report` | Format report (markdown/HTML/JSON) | Management |
| `compare_test_runs` | Diff between two test runs | Management |
| `analyze_coverage` | Code coverage metrics + gaps | Analysis |
| `detect_flaky_tests` | Find intermittently failing tests | Analysis |
| `analyze_test_performance` | Speed analysis, slow tests | Analysis |
| `validate_test_health` | Overall test suite health check | Analysis |

**Total:** 14 tools across 4 categories (Discovery, Execution, Management, Analysis)

---

## Testing Persona: testing-expert

### Specification

**Name:** testing-expert
**Version:** 1.0.0
**Role:** Test Strategy & QA Specialist
**Parent:** null (independent)

**Expertise (15 areas):**
- Test strategy & planning
- Test automation patterns
- Coverage analysis & optimization
- Performance testing & profiling
- Multi-framework testing (pytest, jest, cargo, mocha, vitest)
- CI/CD integration
- Debugging test failures
- Test data management
- Flaky test detection & fixing
- Test reporting & metrics
- Load testing & benchmarking
- Integration & end-to-end testing
- Test optimization techniques
- Regression detection & analysis
- Framework-agnostic testing

**Use Cases (7):**
1. Plan test strategy for new project
2. Debug failing tests across frameworks
3. Analyze coverage gaps
4. Optimize test speed
5. Setup CI/CD testing
6. Detect & fix flaky tests
7. Generate test reports

**System Prompt:** 1500+ lines covering all test patterns for pytest, jest, cargo, mocha, vitest, and custom runners.

### Activation

```bash
use_persona('testing-expert')
```

---

## File Structure

```
coeref-testing/
â”œâ”€â”€ CLAUDE.md                           # This file (AI context)
â”œâ”€â”€ TESTING_GUIDE.md                    # Project vision & implementation roadmap
â”œâ”€â”€ README.md                           # User-facing documentation
â”œâ”€â”€ server.py                           # MCP server entry point
â”œâ”€â”€ pyproject.toml                      # Python package config
â”œâ”€â”€ .claude/commands/                   # Slash commands (12+)
â”‚   â”œâ”€â”€ /run-tests
â”‚   â”œâ”€â”€ /test-results
â”‚   â”œâ”€â”€ /test-report
â”‚   â””â”€â”€ ... (9+ more)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py                       # Test result schemas
â”‚   â”œâ”€â”€ framework_detector.py           # Auto-detect frameworks
â”‚   â”œâ”€â”€ test_runner.py                  # Test execution
â”‚   â”œâ”€â”€ test_aggregator.py              # Result aggregation
â”‚   â”œâ”€â”€ result_analyzer.py              # Coverage/performance/flaky analysis
â”‚   â””â”€â”€ test_coordinator.py             # Multi-project orchestration
â”œâ”€â”€ personas/
â”‚   â””â”€â”€ testing-expert.json             # testing-expert persona definition
â””â”€â”€ coderef/
    â”œâ”€â”€ testing/
    â”‚   â”œâ”€â”€ infrastructure/
    â”‚   â””â”€â”€ results/2025-12-26/
    â””â”€â”€ workorder/
```

---

## Design Decisions

**1. Framework-Agnostic Architecture**
- âœ… Chosen: Support all frameworks (pytest/jest/cargo/mocha/vitest)
- âŒ Rejected: Single-framework focus
- Reason: Maximizes reusability across projects, works with CodeRef + next-scraper + any project

**2. Independent vs Dependent**
- âœ… Chosen: No dependencies on other MCP servers
- âŒ Rejected: Require coderef-context/coderef-workflow
- Reason: Can run standalone, works in any environment, optional integrations when available

**3. Unified Result Schema**
- âœ… Chosen: All frameworks output to standard JSON
- âŒ Rejected: Framework-native result formats
- Reason: Consistent aggregation, easier analysis, framework-agnostic reporting

**4. Async/Parallel Execution**
- âœ… Chosen: Async execution with configurable parallelization
- âŒ Rejected: Sequential test execution
- Reason: Fast feedback, efficient resource use, scales to large test suites

**5. Single Testing Persona**
- âœ… Chosen: testing-expert covers all domains
- âŒ Rejected: Per-framework personas
- Reason: Unified perspective, simpler for users, comprehensive coverage

---

## Integration Guide

### Standalone Usage
coeref-testing works independently. No other MCP servers required.

### With coderef-context (Optional)
Use coderef-context tools to analyze test code dependencies before/after running tests.

### With coderef-workflow (Optional)
Track test suites as workorders in coderef-workflow, use planning tools for test strategy.

### With coderef-docs (Optional)
Generate test reports and documentation using coderef-docs tools.

---

## Essential Commands

### Development

```bash
# Install & run
cd C:\Users\willh\.mcp-servers\coeref-testing
uv sync
python server.py

# Run tests
pytest tests/ -v
mypy src/
```

### Testing / Slash Commands

```bash
/run-tests                    # Run full test suite on project
/run-server-tests             # Run specific test category
/test-results                 # View latest test results
/test-report                  # Generate test report (markdown/HTML/JSON)
/test-coverage                # Show code coverage
/test-trends                  # Show trends & regressions
/test-performance             # Analyze test speed
/detect-flaky                 # Find flaky tests
/test-health                  # Overall health check
/compare-runs                 # Compare two test runs
/discover-tests               # List tests in project
/list-frameworks              # Show detected frameworks
```

---

## Use Cases

### UC-1: Test CodeRef Ecosystem

```
Goal: Run all tests across 4 CodeRef servers
Steps:
1. Use /discover-tests on coderef-context â†’ finds 10 tools to test
2. Use /run-tests on coderef-context â†’ executes all tests
3. Repeat for coderef-workflow, coderef-docs, coderef-personas
4. Use /aggregate-results â†’ summary across all 4 servers
5. Use /test-report â†’ generates ecosystem health report
```

### UC-2: Optimize next-scraper Tests

```
Goal: Make tests faster, fix flaky tests
Steps:
1. Use /test-performance on next-scraper project â†’ identifies slow tests
2. Use /detect-flaky â†’ finds intermittently failing tests
3. Use testing-expert persona â†’ analyze & recommend fixes
4. Run tests again â†’ verify improvements
5. Use /compare-runs â†’ show speed/reliability gains
```

---

## Recent Changes

### v1.0.0 - Initial Planning Phase

- âœ… Comprehensive TESTING_GUIDE.md with architecture & roadmap
- âœ… 14 core tools designed (discovery, execution, analysis, reporting)
- âœ… testing-expert persona specified (15 expertise areas, 7 use cases)
- âœ… Framework-agnostic architecture (pytest, jest, cargo, mocha, vitest)
- âœ… Unified result schema for all frameworks
- âœ… Integration patterns defined (standalone or with other servers)

---

## Next Steps

- â³ Create server skeleton & pyproject.toml
- â³ Implement framework detection (pytest, jest, cargo, mocha, vitest)
- â³ Build test discovery & execution tools (6 tools)
- â³ Implement result aggregation & reporting (4 tools)
- â³ Build analysis tools (4 tools)
- â³ Create slash commands (12+ commands)
- â³ Test on CodeRef ecosystem (4 servers)
- â³ Test on next-scraper project
- â³ Register in global ~/.mcp.json
- â³ Generate usage documentation

---

## Resources

- **[TESTING_GUIDE.md](TESTING_GUIDE.md)** - Complete vision, architecture, roadmap
- **[README.md](README.md)** - User-facing documentation (to be created)
- **[MCP Specification](https://spec.modelcontextprotocol.io/)** - Protocol reference

---

**Maintained by:** willh, Claude Code AI

**System Status:** ğŸš§ Development - Planning phase complete, implementation ready

