---
generated_by: coderef-docs
template: readme
date: "2026-01-14T01:20:48Z"
doc_type: readme
feature_id: foundation-docs
workorder_id: foundation-docs-001
task: Generate foundation documentation
agent: Claude Code AI
_uds:
  validation_score: 95
  validated_at: "2026-01-14T01:20:48Z"
  validator: UDSValidator
---

# CodeRef Context MCP Server

**[Version]** 2.0.0 | **[Date]** 2026-01-14 | **[Maintainer]** willh

## Purpose

CodeRef Context is an MCP (Model Context Protocol) server that provides fast, read-only access to pre-scanned code intelligence. It reads from `.coderef/` directory files generated by the CodeRef dashboard or CLI, eliminating subprocess overhead and providing instant code analysis for AI agents.

## Overview

This MCP server exposes 12 code intelligence tools that enable AI agents to:
- Scan and discover code elements (functions, classes, components, hooks)
- Query code relationships (dependencies, imports, call chains)
- Analyze change impact and complexity
- Discover patterns and test coverage gaps
- Generate comprehensive codebase context
- Export data in multiple formats (JSON, JSON-LD, Mermaid, DOT)

**Key Architecture Decision**: Reads from pre-scanned `.coderef/` files instead of calling CLI subprocesses, resulting in 100x faster response times.

## What

### Core Components

- **MCP Server** (`server.py`): Main entry point exposing 12 MCP tools
- **CodeRef Reader** (`src/coderef_reader.py`): Reads and queries `.coderef/` data files
- **Handlers** (`src/handlers_refactored.py`): Async handlers for each MCP tool
- **Export Processor** (`processors/export_processor.py`): Handles export operations

### MCP Tools Exposed

1. `coderef_scan` - Discover all code elements
2. `coderef_query` - Query relationships (calls, imports, dependencies)
3. `coderef_impact` - Analyze change impact
4. `coderef_complexity` - Get complexity metrics
5. `coderef_patterns` - Discover patterns and test gaps
6. `coderef_coverage` - Test coverage analysis
7. `coderef_context` - Generate comprehensive context
8. `coderef_validate` - Validate CodeRef2 references
9. `coderef_drift` - Detect drift between index and code
10. `coderef_incremental_scan` - Incremental scan (only changed files)
11. `coderef_diagram` - Generate dependency diagrams
12. `coderef_tag` - Add CodeRef2 tags to source files
13. `coderef_export` - Export data in various formats
14. `validate_coderef_outputs` - Validate `.coderef/` files against schemas

## Why

**Problem Solved**: AI agents need fast, reliable access to code intelligence without the overhead of CLI subprocess calls.

**Benefits**:
- **100x Faster**: Direct file reads vs subprocess execution
- **Read-Only Safety**: No code modification, only analysis
- **No External Dependencies**: Works with pre-scanned data
- **MCP Standard**: Follows Model Context Protocol for agent integration

## When

Use this server when:
- Building AI agents that need code intelligence
- Integrating CodeRef analysis into MCP-compatible tools
- Requiring fast, read-only code analysis
- Working with pre-scanned codebases (via dashboard or CLI)

## Prerequisites

- Python 3.10+
- `.coderef/` directory with pre-scanned data:
  - `index.json` (required)
  - `graph.json` (required)
  - `context.json` (required)
  - Optional: `patterns.json`, `coverage.json`, `diagrams/`, `exports/`

## Installation

```bash
# Clone or navigate to project directory
cd coderef-context

# Install dependencies
pip install -e .

# Ensure .coderef/ directory exists with scan data
# (Generated by dashboard scanner or CLI)
```

## Quick Start

### 1. Verify CodeRef Data

```bash
# Check if .coderef/ directory exists
ls .coderef/

# Should contain:
# - index.json
# - graph.json
# - context.json
```

### 2. Start MCP Server

```bash
# Run via stdio (for MCP clients)
python server.py

# Or configure in MCP client settings
```

### 3. Use Tools

Example: Scan codebase
```json
{
  "name": "coderef_scan",
  "arguments": {
    "project_path": "/absolute/path/to/project"
  }
}
```

## Usage Examples

### Example 1: Discover All Code Elements

```python
# MCP tool call
{
  "name": "coderef_scan",
  "arguments": {
    "project_path": "/path/to/project",
    "languages": ["ts", "tsx", "js", "jsx"],
    "use_ast": true
  }
}

# Returns: List of all functions, classes, components, hooks
```

### Example 2: Query Code Relationships

```python
# Find what calls a specific function
{
  "name": "coderef_query",
  "arguments": {
    "project_path": "/path/to/project",
    "query_type": "calls",
    "target": "authenticateUser",
    "max_depth": 3
  }
}
```

### Example 3: Analyze Change Impact

```python
# Check impact of modifying a service
{
  "name": "coderef_impact",
  "arguments": {
    "project_path": "/path/to/project",
    "element": "AuthService",
    "operation": "modify",
    "max_depth": 3
  }
}
```

## Common Issues & Troubleshooting

### Issue: "No scan data found"

**Error Message**:
```json
{
  "success": false,
  "error": "No scan data found. Run scan first to create .coderef/ directory."
}
```

**Resolution**:
1. Run CodeRef scanner via dashboard or CLI
2. Ensure `.coderef/` directory exists in project root
3. Verify `index.json`, `graph.json`, and `context.json` are present

### Issue: "CodeRef data not found: graph.json"

**Error Message**: `FileNotFoundError: CodeRef data not found: graph.json`

**Resolution**:
- Re-run CodeRef scan to generate missing files
- Check file permissions on `.coderef/` directory

### Issue: Corrupted JSON files

**Error**: JSON parsing errors when reading `.coderef/` files

**Resolution**:
- Delete corrupted files and re-scan
- Check disk space and file system integrity

## Architecture

See [ARCHITECTURE.md](coderef/foundation-docs/ARCHITECTURE.md) for detailed system design.

## API Reference

See [API.md](coderef/foundation-docs/API.md) for complete MCP tool documentation.

## Data Models

See [SCHEMA.md](coderef/foundation-docs/SCHEMA.md) for data structure documentation.

## Components

See [COMPONENTS.md](coderef/foundation-docs/COMPONENTS.md) for module documentation.

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run integration tests
pytest tests/test_integration.py

# Run with coverage
pytest --cov=src --cov=processors
```

### Code Quality

```bash
# Format code
black src/ processors/ tests/

# Lint code
ruff check src/ processors/ tests/
```

## License

MIT

## References

- [Model Context Protocol (MCP)](https://modelcontextprotocol.io/)
- [CodeRef Documentation](https://coderef.dev)
- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk)

---

**AI Agent Note**: This MCP server provides read-only code intelligence. All tools are safe to use in automated workflows. For code modification, use `coderef_tag` tool which requires explicit CLI integration.
