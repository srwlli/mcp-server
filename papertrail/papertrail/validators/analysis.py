"""
AnalysisValidator - Validates project analysis JSON files

Validates analysis files generated by coderef-workflow analyze_project_for_planning against:
1. analysis-json-schema.json - Schema structure
2. Additional validation for UDS metadata and data consistency

Scores analysis files 0-100 based on schema compliance and completeness.
"""

from pathlib import Path
from typing import Optional, Union, List, Dict, Any
import json
import re
from jsonschema import Draft7Validator, ValidationError as JsonSchemaValidationError

from ..validator import ValidationResult, ValidationError, ValidationSeverity


class AnalysisValidator:
    """
    Validator for project analysis JSON files

    Validates analysis files against JSON Schema Draft-07 schema and performs
    additional checks for data consistency and UDS compliance.
    """

    def __init__(self, schemas_dir: Optional[Path] = None):
        """
        Initialize validator with schemas directory

        Args:
            schemas_dir: Path to schemas directory (default: package schemas/workflow/)
        """
        if schemas_dir is None:
            # Default to schemas/workflow/ for workflow schemas
            schemas_dir = Path(__file__).parent.parent.parent / "schemas" / "workflow"

        self.schemas_dir = schemas_dir
        self.schema = None
        self._load_schema()

    def _load_schema(self):
        """Load analysis-json-schema.json"""
        schema_path = self.schemas_dir / "analysis-json-schema.json"
        if not schema_path.exists():
            raise FileNotFoundError(f"Schema not found: {schema_path}")

        with open(schema_path, 'r', encoding='utf-8') as f:
            self.schema = json.load(f)

    def validate_file(self, file_path: Union[str, Path]) -> ValidationResult:
        """
        Validate a project analysis JSON file

        Args:
            file_path: Path to analysis.json file

        Returns:
            ValidationResult with errors, warnings, and score
        """
        file_path = Path(file_path)

        if not file_path.exists():
            return ValidationResult(
                valid=False,
                errors=[ValidationError(
                    severity=ValidationSeverity.CRITICAL,
                    message=f"File not found: {file_path}"
                )],
                warnings=[],
                score=0
            )

        # Read JSON content
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            return ValidationResult(
                valid=False,
                errors=[ValidationError(
                    severity=ValidationSeverity.CRITICAL,
                    message=f"Invalid JSON: {str(e)}"
                )],
                warnings=[],
                score=0
            )

        return self.validate_content(data, file_path)

    def validate_content(
        self,
        data: Dict[str, Any],
        file_path: Optional[Path] = None
    ) -> ValidationResult:
        """
        Validate analysis data

        Args:
            data: Analysis data (object)
            file_path: Optional path for context

        Returns:
            ValidationResult with errors, warnings, and score
        """
        errors = []
        warnings = []

        # JSON Schema validation
        validator = Draft7Validator(self.schema)
        schema_errors = list(validator.iter_errors(data))

        for error in schema_errors:
            # Convert jsonschema errors to ValidationError
            path = " -> ".join(str(p) for p in error.path) if error.path else "root"
            errors.append(ValidationError(
                severity=ValidationSeverity.MAJOR,
                message=f"Schema validation failed at {path}: {error.message}",
                field=path
            ))

        # Additional validation checks
        if isinstance(data, dict):
            # Validate _uds metadata
            if '_uds' in data:
                self._validate_uds_metadata(data['_uds'], errors, warnings)

            # Validate inventory data consistency
            if 'inventory_data' in data:
                self._validate_inventory_consistency(data['inventory_data'], errors, warnings)

            # Validate technology stack completeness
            if 'technology_stack' in data:
                self._validate_tech_stack(data['technology_stack'], errors, warnings)

            # Validate foundation docs completeness
            if 'foundation_docs' in data:
                self._validate_foundation_docs(data['foundation_docs'], warnings)

        # Calculate score
        score = self._calculate_score(errors, warnings)

        return ValidationResult(
            valid=score >= 90,
            errors=errors,
            warnings=warnings,
            score=score
        )

    def _validate_uds_metadata(
        self,
        uds_data: Dict[str, Any],
        errors: List[ValidationError],
        warnings: List[str]
    ):
        """
        Validate UDS metadata section

        Args:
            uds_data: _uds metadata object
            errors: List to append errors to
            warnings: List to append warnings to
        """
        # Validate generated_by format
        if 'generated_by' in uds_data:
            generated_by = uds_data['generated_by']
            if not re.match(r'^coderef-workflow v\d+\.\d+\.\d+$', generated_by):
                errors.append(ValidationError(
                    severity=ValidationSeverity.MINOR,
                    message=f"Invalid generated_by format: {generated_by}. Expected: coderef-workflow vX.Y.Z",
                    field="_uds.generated_by"
                ))

        # Validate workorder_id format (if present)
        if 'workorder_id' in uds_data:
            workorder_id = uds_data['workorder_id']
            if not re.match(r'^WO-[A-Z0-9]+-[A-Z0-9]+-\d{3}$', workorder_id):
                errors.append(ValidationError(
                    severity=ValidationSeverity.MAJOR,
                    message=f"Invalid workorder_id format: {workorder_id}. Expected: WO-{{CATEGORY}}-{{ID}}-###",
                    field="_uds.workorder_id"
                ))

        # Validate feature_id format
        if 'feature_id' in uds_data:
            feature_id = uds_data['feature_id']
            if not re.match(r'^[a-z0-9]+(-[a-z0-9]+)*$', feature_id):
                errors.append(ValidationError(
                    severity=ValidationSeverity.MAJOR,
                    message=f"Invalid feature_id format: {feature_id}. Expected: kebab-case",
                    field="_uds.feature_id"
                ))

        # Validate date formats
        for date_field in ['last_updated', 'next_review']:
            if date_field in uds_data:
                date_value = uds_data[date_field]
                if not re.match(r'^\d{4}-\d{2}-\d{2}$', date_value):
                    errors.append(ValidationError(
                        severity=ValidationSeverity.MINOR,
                        message=f"Invalid {date_field} format: {date_value}. Expected: YYYY-MM-DD",
                        field=f"_uds.{date_field}"
                    ))

    def _validate_inventory_consistency(
        self,
        inventory_data: Dict[str, Any],
        errors: List[ValidationError],
        warnings: List[str]
    ):
        """
        Validate inventory data consistency

        Args:
            inventory_data: inventory_data object
            errors: List to append errors to
            warnings: List to append warnings to
        """
        # Check if total_elements matches sum of by_type
        if 'total_elements' in inventory_data and 'by_type' in inventory_data:
            total_declared = inventory_data['total_elements']
            by_type = inventory_data['by_type']

            if isinstance(by_type, dict):
                total_calculated = sum(
                    count for count in by_type.values()
                    if isinstance(count, int)
                )

                # Allow some tolerance (within 10%) due to potential counting differences
                tolerance = max(1, int(total_declared * 0.1))
                if abs(total_declared - total_calculated) > tolerance:
                    warnings.append(
                        f"inventory_data: total_elements ({total_declared}) doesn't match "
                        f"sum of by_type ({total_calculated}). Difference: {abs(total_declared - total_calculated)}"
                    )

        # Warn if source is 'fallback' (indicates data quality issues)
        if 'source' in inventory_data:
            source = inventory_data['source']
            if source == 'fallback':
                warnings.append(
                    "inventory_data: Using 'fallback' source indicates coderef tools were unavailable. "
                    "Data quality may be lower than expected."
                )

    def _validate_tech_stack(
        self,
        tech_stack: Dict[str, Any],
        errors: List[ValidationError],
        warnings: List[str]
    ):
        """
        Validate technology stack completeness

        Args:
            tech_stack: technology_stack object
            errors: List to append errors to
            warnings: List to append warnings to
        """
        # Warn if too many 'unknown' values
        unknown_count = sum(
            1 for value in tech_stack.values()
            if isinstance(value, str) and value == 'unknown'
        )

        if unknown_count >= 3:
            warnings.append(
                f"technology_stack: {unknown_count}/5 fields are 'unknown'. "
                "Consider running deeper project analysis for better tech stack detection."
            )

    def _validate_foundation_docs(
        self,
        foundation_docs: Dict[str, Any],
        warnings: List[str]
    ):
        """
        Validate foundation documentation completeness

        Args:
            foundation_docs: foundation_docs object
            warnings: List to append warnings to
        """
        # Warn if critical docs are missing
        critical_docs = ['README.md', 'ARCHITECTURE.md']

        if 'missing' in foundation_docs and isinstance(foundation_docs['missing'], list):
            missing = foundation_docs['missing']

            for doc in critical_docs:
                if any(doc in missing_item for missing_item in missing):
                    warnings.append(
                        f"foundation_docs: Critical document '{doc}' is missing. "
                        "This may impact planning quality."
                    )

    def _calculate_score(self, errors: List[ValidationError], warnings: List[str]) -> int:
        """
        Calculate validation score (0-100)

        Formula: 100 - 50*CRITICAL - 20*MAJOR - 10*MINOR - 5*WARNING - 2*warnings

        Args:
            errors: List of validation errors
            warnings: List of warning messages

        Returns:
            Score (0-100)
        """
        score = 100

        for error in errors:
            if error.severity == ValidationSeverity.CRITICAL:
                score -= 50
            elif error.severity == ValidationSeverity.MAJOR:
                score -= 20
            elif error.severity == ValidationSeverity.MINOR:
                score -= 10
            elif error.severity == ValidationSeverity.WARNING:
                score -= 5

        score -= 2 * len(warnings)

        return max(0, score)
