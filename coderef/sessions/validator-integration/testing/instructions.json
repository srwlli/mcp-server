{
  "session_id": "validator-integration-testing",
  "phase": "Integration Verification",
  "created": "2026-01-10",
  "purpose": "Verify coderef-workflow validator integration by running workflow integration tests that were previously XFAIL",

  "orchestrator_instructions": {
    "task": "Verify testing results after coderef-workflow and coderef-docs complete integration",
    "prerequisite_status": "✅ BOTH COMPLETE - coderef-workflow (4/4 gaps) and coderef-docs (direct validation) done",
    "steps": [
      "Read coderef-testing integration-verification-report.json",
      "Verify 8/9 integration tests pass (all except GAP-003)",
      "Verify 1/9 test remains XFAIL (GAP-003 not applicable)",
      "Verify total test count: 23/24 passing (15 unit + 8/9 integration), 1 XFAIL",
      "Verify no unexpected failures",
      "Create orchestrator-testing-verification.md",
      "Update parent session communication.json",
      "Mark validator-integration session as complete"
    ],
    "success_criteria": [
      "8/9 integration tests pass (all GAP-001, 002, 004, 005 tests verified)",
      "1/9 integration test XFAIL documented (GAP-003 not applicable)",
      "No regression in 15 unit tests",
      "Total: 23/24 passing, 1 XFAIL",
      "Integration fully verified (4/4 applicable gaps complete)"
    ]
  },

  "agent_instructions": {
    "agent_id": "coderef-testing",
    "prerequisite": "✅ READY: coderef-workflow and coderef-docs have completed integration",
    "task": "Run test_workflow_integration.py and verify integration test results after GAP implementation",

    "context": {
      "background": "During WO-PAPERTRAIL-SCHEMA-ADDITIONS-001 testing, we created 9 integration tests that are marked XFAIL because validators weren't integrated into workflows yet. After coderef-workflow and coderef-docs implemented integration, we need to verify test results.",
      "implementation_status": {
        "coderef_workflow": "4/4 gaps complete (GAP-001, GAP-002, GAP-004, GAP-005) - all 19 validator sites refactored to ValidatorFactory",
        "coderef_docs": "Direct validation integration complete (8 templates validated with _uds metadata)",
        "current_test_status": "15/15 unit tests passing, 5/9 integration tests passing, 4 XFAIL remain"
      },
      "test_file": "C:\\Users\\willh\\.mcp-servers\\papertrail\\tests\\validators\\test_workflow_integration.py",
      "expected_result": "Verify 8/9 integration tests pass (all except GAP-003), 1 XFAIL remains (update_task_status function doesn't exist)"
    },

    "step_1_verify_prerequisite": {
      "description": "✅ PREREQUISITE MET - Review completed implementation",
      "status": "Both agents complete - ready to verify",
      "actions": [
        "Read parent session communication.json (verify coderef-workflow and coderef-docs status = 'complete')",
        "Read C:\\Users\\willh\\.mcp-servers\\coderef\\sessions\\validator-integration\\coderef-workflow-audit.json",
        "Review implementation notes: 3/4 gaps complete, GAP-004 partial (2/19 sites), 5/9 integration tests passing",
        "Understand which tests should pass vs remain XFAIL based on gaps completed"
      ]
    },

    "step_2_review_integration": {
      "description": "Review what was implemented by both agents",
      "coderef_workflow_completed": {
        "gaps_complete": ["GAP-001", "GAP-002", "GAP-004", "GAP-005"],
        "gap_004_details": "19/19 sites refactored to ValidatorFactory (6 generators, handler_helpers.py, tool_handlers.py)",
        "gaps_not_applicable": ["GAP-003 (update_task_status function doesn't exist in coderef-workflow)"],
        "files_modified": ["utils/validation_helpers.py (NEW)", "tool_handlers.py", "handler_helpers.py", "generators/* (6 files)", "tests/validators/test_workflow_integration.py"],
        "commits": "7 commits (588e070 = GAP-004 complete)",
        "tests_passing": "15/15 unit, 5/9 integration (4 XFAIL before GAP-004 completion)"
      },
      "coderef_docs_completed": {
        "gaps_complete": ["GAP-001 (direct validation)"],
        "validated_templates": ["readme, architecture, api, schema, components (foundation)", "ui-patterns, behavior-patterns, ux-patterns (standards)"],
        "validation_pattern": "Direct integration (tool saves files, runs validators, writes _uds metadata)",
        "tests_passing": "6/6 new direct validation tests"
      }
    },

    "step_3_run_integration_tests": {
      "description": "Run integration tests and document results based on gaps completed",
      "commands": [
        "cd C:\\Users\\willh\\.mcp-servers\\papertrail",
        "pytest tests/validators/test_workflow_integration.py -v --tb=short"
      ],
      "test_expectations": {
        "passing_tests": [
          "test_analyze_project_calls_validator (GAP-001 complete)",
          "test_analysis_output_includes_validation_metadata (GAP-001 complete)",
          "test_execute_plan_calls_validator (GAP-002 complete)",
          "test_execute_plan_enables_cross_validation (GAP-002 complete)",
          "test_workflows_use_factory_for_auto_detection (GAP-004 complete - 19/19 sites refactored)",
          "test_workflow_warns_on_low_validation_score (GAP-005 complete)",
          "test_workflow_continues_with_warnings (GAP-005 complete)",
          "test_workflow_rejects_critical_failures (GAP-005 complete)"
        ],
        "xfail_tests": [
          "test_update_task_status_validates_before_update (GAP-003 not applicable - update_task_status function doesn't exist in coderef-workflow)"
        ]
      },
      "expected_result": "8/9 passing, 1 XFAIL (GAP-003 not applicable)"
    },

    "step_4_verify_no_regression": {
      "description": "Ensure unit tests still pass",
      "commands": [
        "pytest tests/validators/test_validators.py -v",
        "pytest tests/validators/test_factory.py -v"
      ],
      "expected_result": "20/20 unit tests passing (no regression)"
    },

    "step_5_run_full_test_suite": {
      "description": "Run all validator tests together",
      "commands": [
        "pytest tests/validators/ -v --tb=short"
      ],
      "expected_result": "23/24 tests passing (15/15 unit + 8/9 integration), 1 XFAIL (GAP-003)"
    },

    "step_6_verify_coverage": {
      "description": "Check test coverage unchanged",
      "commands": [
        "pytest tests/validators/ --cov=papertrail.validators --cov-report=term"
      ],
      "expected_coverage": {
        "analysis_validator": "80%+ (same as before)",
        "execution_log_validator": "87%+ (same as before)",
        "validator_factory": "High coverage"
      }
    },

    "step_7_create_verification_report": {
      "description": "Document test results",
      "output_files": [
        "integration-verification-report.json",
        "integration-verification-report.md"
      ],
      "report_structure": {
        "summary": {
          "total_tests": 24,
          "passing": 23,
          "xfail": 1,
          "failing": 0,
          "xfail_removed": 8,
          "regression": false
        },
        "integration_tests": {
          "test_analyze_project_calls_validator": "PASS (GAP-001)",
          "test_analysis_output_includes_validation_metadata": "PASS (GAP-001)",
          "test_execute_plan_calls_validator": "PASS (GAP-002)",
          "test_execute_plan_enables_cross_validation": "PASS (GAP-002)",
          "test_workflows_use_factory_for_auto_detection": "PASS (GAP-004 - 19/19 sites refactored)",
          "test_workflow_warns_on_low_validation_score": "PASS (GAP-005)",
          "test_workflow_continues_with_warnings": "PASS (GAP-005)",
          "test_workflow_rejects_critical_failures": "PASS (GAP-005)",
          "test_update_task_status_validates_before_update": "XFAIL (GAP-003 not applicable - function doesn't exist)"
        },
        "unit_tests": {
          "test_validators.py": "15/15 passing",
          "test_factory.py": "5/5 passing"
        },
        "coverage": {
          "analysis_validator": "XX%",
          "execution_log_validator": "XX%",
          "validator_factory": "XX%"
        },
        "gaps_verified": [
          "GAP-001: analyze_project_for_planning calls AnalysisValidator ✅ COMPLETE",
          "GAP-002: execute_plan calls ExecutionLogValidator with cross-validation ✅ COMPLETE",
          "GAP-003: update_task_status validates before updating ❌ NOT APPLICABLE (function doesn't exist in coderef-workflow)",
          "GAP-004: Workflows use ValidatorFactory for auto-detection ✅ COMPLETE (19/19 sites refactored)",
          "GAP-005: Consistent error handling implemented ✅ COMPLETE"
        ],
        "integration_status": "COMPLETE - 4/4 applicable gaps implemented (GAP-003 not applicable)"
      }
    },

    "step_8_update_communication_json": {
      "description": "Mark testing complete",
      "actions": [
        "Update testing/communication.json status to 'complete'",
        "Add completion notes with test results",
        "Update aggregation counts"
      ]
    }
  },

  "if_tests_fail": {
    "description": "What to do if integration tests still fail",
    "actions": [
      "Document which tests failed and why",
      "Identify which gaps are not fully integrated",
      "Create detailed failure report with error messages",
      "Notify orchestrator and coderef-workflow agent",
      "Wait for coderef-workflow to fix issues",
      "Re-run tests after fixes"
    ],
    "report_format": {
      "failed_tests": ["List of test names"],
      "failure_reasons": ["Specific error messages"],
      "gaps_not_fixed": ["GAP-XXX IDs"],
      "recommended_fixes": ["Specific code changes needed"]
    }
  },

  "success_criteria": {
    "critical": [
      "8/9 integration tests pass (all tests except GAP-003)",
      "1/9 integration test XFAIL (GAP-003 not applicable - function doesn't exist)",
      "No regression in 15 unit tests (15/15 passing)",
      "Total: 23/24 tests passing, 1 XFAIL (GAP-003)",
      "No unexpected failures"
    ],
    "optional": [
      "Coverage maintained or improved",
      "Test execution time < 10 seconds",
      "No warnings in test output"
    ]
  },

  "reference_documents": [
    "C:\\Users\\willh\\.mcp-servers\\coderef\\sessions\\archived\\papertrail-uds-alignment\\testing\\validator-integration-gap-report.md",
    "C:\\Users\\willh\\.mcp-servers\\papertrail\\tests\\validators\\test_workflow_integration.py",
    "Parent session: C:\\Users\\willh\\.mcp-servers\\coderef\\sessions\\validator-integration\\communication.json"
  ]
}
